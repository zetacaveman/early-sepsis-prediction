# early-sepsis-prediction

Temporal Convolutional Network (TCN) for ICU Time-Series (PhysioNet 2019 Sepsis)



## Data assumptions

Tabular ICU time series with columns like:

	•	PatientID, Hour (integer hours since admission),
	•	feature columns (vitals/labs),
	•	label column SepsisLabel (0/1).
	•	Variable length per patient (some have 4 hours, others 45+, etc.).
	•	Optional: parquet files (.parquet) for faster I/O.

Note from EDA (done by other team member): 
1. Many datasets have a bookkeeping row at Hour = 0 that’s mostly empty. We usually drop Hour==0 so sequences start at 1.

2. Different patients have different hours of observation. In this case, we use padding to address time series data of different size. 

3. The final prediction is about whether a patient ONCE have sepsis. 

## Environment

python >= 3.9
pytorch >= 2.0
pandas
numpy
scikit-learn
pyarrow        # if reading parquet, used in other part. 




## Rough ideas

Padding + masking: pad sequences to same length per batch.

Normalization: compute mean/std on training set only.

Loss: BCEWithLogitsLoss (raw logits input).

Imbalance: optional class weighting

valuation: AUROC/AUPRC (threshold-free) + confusion matrix.

## What is TCN?

A Temporal Convolutional Network (TCN) is a type of 1-D convolutional neural network designed for sequence and time-series data.
Unlike recurrent networks (LSTM, GRU), which process data step by step, TCNs use causal and dilated convolutions to capture both short- and long-term temporal dependencies in parallel.

Key properties:
	•	Causal convolutions: ensure predictions at time t depend only on times ≤ t (no future leakage).
	•	Dilations: skip over timesteps to efficiently cover long histories without deep recurrence.
	•	Residual blocks: help gradient flow and stabilize training for long sequences.

TCNs often train faster and are easier to parallelize than RNNs, making them a strong baseline for many sequence modeling tasks such as speech, sensor, and medical time series.

# Key components

Most part of codes is generated by ChatGPT, with some modifications for our project. 
## Load + normalize

```python
def load_and_normalize(train_csv, val_csv):
    # 1) load both
    df_train = pd.read_csv(train_csv)
    df_val   = pd.read_csv(val_csv)

    # 2) sort
    df_train = df_train.sort_values(["PatientID", "Hour"]).reset_index(drop=True)
    df_val   = df_val.sort_values(["PatientID", "Hour"]).reset_index(drop=True)

    # 3) OPTIONAL: drop Hour == 0
    df_train = df_train[df_train["Hour"] > 0]
    df_val   = df_val[df_val["Hour"] > 0]

    # 4) compute normalization stats **on train only**
    train_means = df_train[FEATURE_COLS].mean()
    train_stds  = df_train[FEATURE_COLS].std().replace(0, 1e-6)

    # 5) apply to both train and val
    df_train[FEATURE_COLS] = (df_train[FEATURE_COLS] - train_means) / train_stds
    df_val[FEATURE_COLS]   = (df_val[FEATURE_COLS] - train_means) / train_stds

    return df_train, df_val, train_means, train_stds
```

## Build patient sequences
```python
def df_to_patients(df):
    patients = []
    for pid, g in df.groupby("PatientID"):
        g = g.sort_values("Hour")
        
        # Skip the 'bookkeeping' hour
        g = g[g['Hour'] > 0]
        if g.empty:
            continue
        
        # patient-level label: ever septic
        y = int(g[LABEL_COL].fillna(0).max())

        # take feature columns and fill NaNs
        X = g[FEATURE_COLS].copy()
        X = X.fillna(0.0)   # <— very important. We assume the dataset is processed, this is for debug

        # convert to tensor [T, F]
        X_tensor = torch.tensor(
            X.to_numpy(dtype="float32"),
            dtype=torch.float32,
        )

        patients.append({
            "patient_id": pid,
            "series": X_tensor,          # [T, F]
            "label": torch.tensor(y).float(),
            "length": X_tensor.shape[0],
        })
    in_features = len(FEATURE_COLS)
    return patients, in_features
```

## Dataset + collate (padding)
```python
class TCNDataset(Dataset):
    def __init__(self, patients): self.patients = patients
    def __len__(self): return len(self.patients)
    def __getitem__(self, i): return self.patients[i]

def collate_tcn(batch):
    lengths = [b['length'] for b in batch]
    max_len = max(lengths)
    feat_dim = batch[0]['series'].shape[1]
    feats = torch.zeros(len(batch), max_len, feat_dim)
    mask  = torch.zeros(len(batch), max_len)
    labels = torch.zeros(len(batch))
    last_idx = torch.zeros(len(batch), dtype=torch.long)

    for i,b in enumerate(batch):
        T = b['length']
        feats[i,:T,:] = b['series']
        mask[i,:T] = 1.0
        labels[i] = b['label']
        last_idx[i] = T-1
    feats = feats.permute(0,2,1)
    return {'x':feats,'mask':mask,'y':labels,'last_idx':last_idx}
```
## TCN model


```python
class TemporalBlock(nn.Module):
    def __init__(self, in_ch, out_ch, kernel_size, dilation, dropout=0.1):
        super().__init__()
        
        # causal padding so output length == input length
        pad = (kernel_size - 1) * dilation

        self.conv1 = nn.Conv1d(in_ch, out_ch,
                               kernel_size=kernel_size,
                               padding=pad,
                               dilation=dilation)
        self.relu1 = nn.ReLU()
        self.drop1 = nn.Dropout(dropout)

        self.conv2 = nn.Conv1d(out_ch, out_ch,
                               kernel_size=kernel_size,
                               padding=pad,
                               dilation=dilation)
        self.relu2 = nn.ReLU()
        self.drop2 = nn.Dropout(dropout)

        # 1x1 conv for residual if channel dims change
        self.residual = (
            nn.Conv1d(in_ch, out_ch, kernel_size=1)
            if in_ch != out_ch else nn.Identity()
        )


        self._init_weights()

    def _init_weights(self):
        for m in [self.conv1, self.conv2]:
            nn.init.kaiming_normal_(m.weight)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
        if isinstance(self.residual, nn.Conv1d):
            nn.init.kaiming_normal_(self.residual.weight)
            if self.residual.bias is not None:
                nn.init.zeros_(self.residual.bias)

    def forward(self, x):
        # x: [B, C, T]
        T = x.size(-1)

        y = self.conv1(x)[:, :, :T]
        y = self.relu1(y)
        y = self.drop1(y)

        y = self.conv2(y)[:, :, :T]
        y = self.relu2(y)
        y = self.drop2(y)

        res = self.residual(x)[:, :, :T]

        return y + res  # residual

# Now stack several TemporalBlocks with increasing dilation rates:

class TCNModel(nn.Module):
    def __init__(self, in_ch, hidden_ch=64, num_levels=4, kernel_size=3):
        super().__init__()
        layers = []
        ch_in = in_ch
        for i in range(num_levels):
            dilation = 2 ** i  # 1,2,4,8,...
            ch_out = hidden_ch
            layers.append(
                TemporalBlock(
                    in_ch=ch_in,
                    out_ch=ch_out,
                    kernel_size=kernel_size,
                    dilation=dilation,
                    dropout=0.1,
                )
            )
            ch_in = ch_out

        self.tcn = nn.Sequential(*layers)
        self.head = nn.Conv1d(ch_in, 1, kernel_size=1)  # per-timestep logit

    def forward(self, x):
        # x: [B, in_ch, T]
        h = self.tcn(x)        # [B, hidden_ch, T]
        logits = self.head(h)  # [B, 1, T]
        logits = logits.squeeze(1)  # [B, T]
        return logits
```
## Training step

```python
def train_step(model, batch, optimizer, device, pos_weight):
    model.train()
    x = batch["x"].to(device)             # [B, C, T]
    y = batch["y"].to(device)             # [B]
    last_idx = batch["last_idx"].to(device)

    logits_time = model(x)                # [B, T]
    
    # gather last valid timestep logits
    # shape [B]
    B = logits_time.shape[0]
    logits_last = logits_time[torch.arange(B, device=device), last_idx]

    if torch.isnan(logits_last).any():
        print("NaN in logits_last!")
        print("logits_last:", logits_last)
    if torch.isnan(y).any():
        print("NaN in labels!")
    
    # compute class ratio. Use Weighted loss to handle imbalance in neural network
    #num_pos = sum(p["label"].item() for p in train_patients)
    #num_neg = len(train_patients) - num_pos
    #pos_weight = torch.tensor(num_neg / num_pos)
    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))
    #loss_fn = nn.BCEWithLogitsLoss()
    loss = loss_fn(logits_last, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    with torch.no_grad():
        probs = torch.sigmoid(logits_last)
        preds = (probs > 0.5).float()
        acc = (preds == y).float().mean()

    return loss.item(), acc.item()
```
## Evaluation (AUROC, AUPRC, CM, Recall)


```python
@torch.no_grad()
def eval_epoch(model, loader, device):
    model.eval()
    all_probs, all_labels = [], []

    total_loss = 0.0
    count = 0
    loss_fn = nn.BCEWithLogitsLoss()

    for batch in loader:
        x = batch["x"].to(device)
        y = batch["y"].to(device)
        last_idx = batch["last_idx"].to(device)

        logits_time = model(x)             # [B, T]
        B = logits_time.shape[0]
        logits_last = logits_time[torch.arange(B, device=device), last_idx]  # [B]
        loss = loss_fn(logits_last, y)

        total_loss += loss.item()
        count += 1

        probs = torch.sigmoid(logits_last).detach().cpu().numpy()
        labels = y.detach().cpu().numpy()
        all_probs.extend(probs)
        all_labels.extend(labels)

    try:
        auroc = roc_auc_score(all_labels, all_probs)
    except ValueError:
        auroc = float('nan')
    try:
        auprc = average_precision_score(all_labels, all_probs)
    except ValueError:
        auprc = float('nan')

    return {
        "val_loss": total_loss / max(1, count),
        "val_auroc": auroc,
        "val_auprc": auprc,
    }
```

## Training loop (timed)

```python
for epoch in range(10):
    # ----- TRAIN -----
    batch_losses = []
    batch_accs = []
    for batch in train_loader:
        loss, acc = train_step(model, batch, optimizer, device, pos_weight)
        batch_losses.append(loss)
        batch_accs.append(acc)
    avg_loss = sum(batch_losses) / len(batch_losses)
    avg_acc  = sum(batch_accs) / len(batch_accs)
    print(f"epoch {epoch} | train_loss={avg_loss:.4f} | train_acc={avg_acc:.4f}")

    # ----- Validation/Test -----
    val_metrics = eval_epoch(model, val_loader, device)
    print(f"             | Val loss={val_metrics['val_loss']:.4f} "
          f"AUROC={val_metrics['val_auroc']:.3f} AUPRC={val_metrics['val_auprc']:.3f}")

```


# Notes
	•	NaN loss → fill missing data, check std>0.
	•	Accuracy = 1.0 suddenly → probably only last batch printed.
	•	Imbalance: start unweighted; try pos_weight ≤ 5 if recall too low.
	•	Dropout: increase to ~0.3 if overfitting.


## Future work

The Temporal Convolutional Network (TCN) provided a stable baseline for sequential ICU data, but its predictive performance lagged behind other architectures.

Future work can focus on a few key directions:

1.	Refine class imbalance handling:
Test focal loss or smarter sampling instead of a fixed positive weight.

2.	Evaluate clinically:
Use early-warning prediction horizons (e.g., predicting sepsis 6 h ahead) and assess calibration or interpretability of predictions.


## Reference


https://physionet.org/content/challenge-2019/1.0.0/


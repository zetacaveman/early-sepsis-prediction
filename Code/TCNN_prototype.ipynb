{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2aea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 0: Import\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92908147",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Prep helper: make patient-level sequences\n",
    "## We:\n",
    "## read the CSV,\n",
    "## choose which columns are model inputs (features),\n",
    "## forward-fill + impute missing values,\n",
    "## also create a parallel mask of which values were actually observed (informative missingness),\n",
    "## store each patient as a dict.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"HR\", \"O2Sat\", \"Temp\", \"SBP\", \"MAP\", \"Resp\", \"pH\",\n",
    "    \"BUN\", \"WBC\", \"Platelets\", \"Magnesium\", \"Potassium\"\n",
    "]\n",
    "LABEL_COL = \"SepsisLabel\"\n",
    "META_COLS = [\"PatientID\", \"Hour\"]\n",
    "\n",
    "\n",
    "def load_and_normalize(train_csv, val_csv):\n",
    "    # 1) load both\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    df_val   = pd.read_csv(val_csv)\n",
    "\n",
    "    # 2) sort\n",
    "    df_train = df_train.sort_values([\"PatientID\", \"Hour\"]).reset_index(drop=True)\n",
    "    df_val   = df_val.sort_values([\"PatientID\", \"Hour\"]).reset_index(drop=True)\n",
    "\n",
    "    # 3) OPTIONAL: drop Hour == 0\n",
    "    df_train = df_train[df_train[\"Hour\"] > 0]\n",
    "    df_val   = df_val[df_val[\"Hour\"] > 0]\n",
    "\n",
    "    # 4) compute normalization stats **on train only**\n",
    "    train_means = df_train[FEATURE_COLS].mean()\n",
    "    train_stds  = df_train[FEATURE_COLS].std().replace(0, 1e-6)\n",
    "\n",
    "    # 5) apply to both train and val\n",
    "    df_train[FEATURE_COLS] = (df_train[FEATURE_COLS] - train_means) / train_stds\n",
    "    df_val[FEATURE_COLS]   = (df_val[FEATURE_COLS] - train_means) / train_stds\n",
    "\n",
    "    return df_train, df_val, train_means, train_stds\n",
    "\n",
    "def load_and_normalize_one(csv_path): #just for testing\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # drop the bookkeeping hour\n",
    "    df = df[df[\"Hour\"] > 0].sort_values([\"PatientID\", \"Hour\"]).reset_index(drop=True)\n",
    "\n",
    "    # compute stats on this big file\n",
    "    means = df[FEATURE_COLS].mean()\n",
    "    stds  = df[FEATURE_COLS].std().replace(0, 1e-6)\n",
    "\n",
    "    df[FEATURE_COLS] = (df[FEATURE_COLS] - means) / stds\n",
    "    return df\n",
    "\n",
    "def build_patient_sequences_simple(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.sort_values([\"PatientID\", \"Hour\"]).reset_index(drop=True)\n",
    "\n",
    "    # Keep only what we care about\n",
    "    keep_cols = META_COLS + FEATURE_COLS + [LABEL_COL]\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    patients = []\n",
    "\n",
    "    # Precompute global means for fallback impute at the end\n",
    "    global_means = df[FEATURE_COLS].mean(skipna=True)\n",
    "\n",
    "    for pid, g in df.groupby(\"PatientID\"):\n",
    "        g = g.sort_values(\"Hour\")\n",
    "        \n",
    "        # Skip the 'bookkeeping' hour\n",
    "        g = g[g['Hour'] > 0]\n",
    "        \n",
    "\n",
    "        # Patient label: septic if ever SepsisLabel==1 i.e. ever septic\n",
    "        y_patient = int(g[LABEL_COL].fillna(0).max())\n",
    "\n",
    "        # Grab features only\n",
    "        X = g[FEATURE_COLS].copy()\n",
    "        X = X.to_numpy(dtype=float)\n",
    "\n",
    "        ## 1. forward fill within patient\n",
    "        ##X = X.ffill()\n",
    "\n",
    "        ## 2. fill any still-missing with global means\n",
    "        ##X = X.fillna(global_means)\n",
    "\n",
    "        # convert to tensor [T, F]\n",
    "        X_tensor = torch.tensor(X.to_numpy(dtype=np.float32))\n",
    "\n",
    "        patients.append({\n",
    "            \"patient_id\": pid,\n",
    "            \"series\": X_tensor,                        # [T, F]\n",
    "            \"label\": torch.tensor(y_patient).float(), # scalar\n",
    "            \"length\": X_tensor.shape[0],\n",
    "        })\n",
    "\n",
    "     # Also return feature size for model building\n",
    "    in_features = len(FEATURE_COLS)\n",
    "    return patients, in_features\n",
    "\n",
    "def df_to_patients(df):\n",
    "    patients = []\n",
    "    for pid, g in df.groupby(\"PatientID\"):\n",
    "        g = g.sort_values(\"Hour\")\n",
    "        \n",
    "        # Skip the 'bookkeeping' hour\n",
    "        g = g[g['Hour'] > 0]\n",
    "        if g.empty:\n",
    "            continue\n",
    "        \n",
    "        # patient-level label: ever septic\n",
    "        y = int(g[LABEL_COL].fillna(0).max())\n",
    "\n",
    "        # take feature columns and fill NaNs\n",
    "        X = g[FEATURE_COLS].copy()\n",
    "        X = X.fillna(0.0)   # <— very important. We assume the dataset is processed, this is for debug\n",
    "\n",
    "        # convert to tensor [T, F]\n",
    "        X_tensor = torch.tensor(\n",
    "            X.to_numpy(dtype=\"float32\"),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "\n",
    "        patients.append({\n",
    "            \"patient_id\": pid,\n",
    "            \"series\": X_tensor,          # [T, F]\n",
    "            \"label\": torch.tensor(y).float(),\n",
    "            \"length\": X_tensor.shape[0],\n",
    "        })\n",
    "    in_features = len(FEATURE_COLS)\n",
    "    return patients, in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b6a9e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: PyTorch Dataset for patients, This is basically a light wrapper around that list.\n",
    "class TCNDataset(Dataset):\n",
    "    def __init__(self, patients_list):\n",
    "        self.patients = patients_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.patients[idx]\n",
    "        return {\n",
    "            \"series\": p[\"series\"],        # [T, F]\n",
    "            \"label\": p[\"label\"],          # scalar\n",
    "            \"length\": p[\"length\"],        # int\n",
    "            \"patient_id\": p[\"patient_id\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "039a5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Collate function for variable-length padding\n",
    "## finds the max length in the batch,\n",
    "## pads everyone to that length,\n",
    "## builds a mask so we know which timesteps are real,\n",
    "## permutes to [B, C, T] for TCNs.\n",
    "\n",
    "\n",
    "def collate_tcn(batch):\n",
    "    lengths = [b[\"length\"] for b in batch]\n",
    "    max_len = max(lengths)\n",
    "    feat_dim = batch[0][\"series\"].shape[1]\n",
    "    B = len(batch)\n",
    "\n",
    "    feats = torch.zeros(B, max_len, feat_dim, dtype=torch.float32)\n",
    "    time_mask = torch.zeros(B, max_len, dtype=torch.float32)\n",
    "    labels = torch.zeros(B, dtype=torch.float32)\n",
    "    last_idx = torch.zeros(B, dtype=torch.long)\n",
    "\n",
    "    patient_ids = []\n",
    "\n",
    "    for i, b in enumerate(batch):\n",
    "        T = b[\"length\"]\n",
    "        feats[i, :T, :] = b[\"series\"]\n",
    "        time_mask[i, :T] = 1.0\n",
    "        labels[i] = b[\"label\"]\n",
    "        last_idx[i] = T - 1\n",
    "        patient_ids.append(b[\"patient_id\"])\n",
    "\n",
    "    # TCN expects [B, C, T]\n",
    "    feats = feats.permute(0, 2, 1)\n",
    "\n",
    "    return {\n",
    "        \"x\": feats,            # [B, feat_dim, max_len]\n",
    "        \"mask\": time_mask,     # [B, max_len] (not strictly needed for patient-level loss)\n",
    "        \"y\": labels,           # [B]\n",
    "        \"last_idx\": last_idx,  # [B]\n",
    "        \"patient_id\": patient_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5db446da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: A minimal TCN model\n",
    "\n",
    "## This is a pretty standard residual dilated 1D-conv block.\n",
    "## Key details:\n",
    "## Causal conv (padding = dilation*(kernel_size-1)) so we don’t “peek into the future.”\n",
    "## Residual connection so gradients flow.\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, dilation, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # causal padding so output length == input length\n",
    "        pad = (kernel_size - 1) * dilation\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_ch, out_ch,\n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=pad,\n",
    "                               dilation=dilation)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch,\n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=pad,\n",
    "                               dilation=dilation)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        # 1x1 conv for residual if channel dims change\n",
    "        self.residual = (\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size=1)\n",
    "            if in_ch != out_ch else nn.Identity()\n",
    "        )\n",
    "\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in [self.conv1, self.conv2]:\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        if isinstance(self.residual, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(self.residual.weight)\n",
    "            if self.residual.bias is not None:\n",
    "                nn.init.zeros_(self.residual.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, T]\n",
    "        T = x.size(-1)\n",
    "\n",
    "        y = self.conv1(x)[:, :, :T]\n",
    "        y = self.relu1(y)\n",
    "        y = self.drop1(y)\n",
    "\n",
    "        y = self.conv2(y)[:, :, :T]\n",
    "        y = self.relu2(y)\n",
    "        y = self.drop2(y)\n",
    "\n",
    "        res = self.residual(x)[:, :, :T]\n",
    "\n",
    "        return y + res  # residual\n",
    "\n",
    "# Now stack several TemporalBlocks with increasing dilation rates:\n",
    "\n",
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, in_ch, hidden_ch=64, num_levels=4, kernel_size=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        ch_in = in_ch\n",
    "        for i in range(num_levels):\n",
    "            dilation = 2 ** i  # 1,2,4,8,...\n",
    "            ch_out = hidden_ch\n",
    "            layers.append(\n",
    "                TemporalBlock(\n",
    "                    in_ch=ch_in,\n",
    "                    out_ch=ch_out,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    dropout=0.1,\n",
    "                )\n",
    "            )\n",
    "            ch_in = ch_out\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.head = nn.Conv1d(ch_in, 1, kernel_size=1)  # per-timestep logit\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, in_ch, T]\n",
    "        h = self.tcn(x)        # [B, hidden_ch, T]\n",
    "        logits = self.head(h)  # [B, 1, T]\n",
    "        logits = logits.squeeze(1)  # [B, T]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6c40c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Training step (patient-level label)\n",
    "## We’ll treat the patient label as “risk at last observed hour.”\n",
    "## So:\n",
    "## run model → [B, T]\n",
    "## pick each patient’s last_idx\n",
    "## compute sigmoid + BCEWithLogitsLoss\n",
    "\n",
    "\n",
    "def train_step(model, batch, optimizer, device, pos_weight):\n",
    "    model.train()\n",
    "    x = batch[\"x\"].to(device)             # [B, C, T]\n",
    "    y = batch[\"y\"].to(device)             # [B]\n",
    "    last_idx = batch[\"last_idx\"].to(device)\n",
    "\n",
    "    logits_time = model(x)                # [B, T]\n",
    "    \n",
    "    # gather last valid timestep logits\n",
    "    # shape [B]\n",
    "    B = logits_time.shape[0]\n",
    "    logits_last = logits_time[torch.arange(B, device=device), last_idx]\n",
    "\n",
    "    if torch.isnan(logits_last).any():\n",
    "        print(\"NaN in logits_last!\")\n",
    "        print(\"logits_last:\", logits_last)\n",
    "    if torch.isnan(y).any():\n",
    "        print(\"NaN in labels!\")\n",
    "    \n",
    "    # compute class ratio. Use Weighted loss to handle imbalance in neural network\n",
    "    #num_pos = sum(p[\"label\"].item() for p in train_patients)\n",
    "    #num_neg = len(train_patients) - num_pos\n",
    "    #pos_weight = torch.tensor(num_neg / num_pos)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "    #loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss = loss_fn(logits_last, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits_last)\n",
    "        preds = (probs > 0.5).float()\n",
    "        acc = (preds == y).float().mean()\n",
    "\n",
    "    return loss.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7fbc2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation step\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[\"x\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "        last_idx = batch[\"last_idx\"].to(device)\n",
    "\n",
    "        logits_time = model(x)             # [B, T]\n",
    "        B = logits_time.shape[0]\n",
    "        logits_last = logits_time[torch.arange(B, device=device), last_idx]  # [B]\n",
    "        loss = loss_fn(logits_last, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "        probs = torch.sigmoid(logits_last).detach().cpu().numpy()\n",
    "        labels = y.detach().cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auroc = float('nan')\n",
    "    try:\n",
    "        auprc = average_precision_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auprc = float('nan')\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": total_loss / max(1, count),\n",
    "        \"val_auroc\": auroc,\n",
    "        \"val_auprc\": auprc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "940594b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight = tensor([12.3333])\n",
      "epoch 0 | train_loss=3.0859 | train_acc=0.6667\n",
      "             | Val loss=0.6355 AUROC=0.396 AUPRC=0.080\n",
      "epoch 1 | train_loss=1.9570 | train_acc=0.7292\n",
      "             | Val loss=1.1688 AUROC=0.405 AUPRC=0.080\n",
      "epoch 2 | train_loss=1.0016 | train_acc=0.5000\n",
      "             | Val loss=0.8147 AUROC=0.550 AUPRC=0.103\n",
      "epoch 3 | train_loss=0.5999 | train_acc=0.7500\n",
      "             | Val loss=0.4267 AUROC=0.631 AUPRC=0.124\n",
      "epoch 4 | train_loss=0.3023 | train_acc=0.8750\n",
      "             | Val loss=0.3162 AUROC=0.721 AUPRC=0.166\n",
      "epoch 5 | train_loss=0.3674 | train_acc=0.9167\n",
      "             | Val loss=0.3096 AUROC=0.730 AUPRC=0.167\n",
      "epoch 6 | train_loss=0.3362 | train_acc=0.9375\n",
      "             | Val loss=0.3108 AUROC=0.739 AUPRC=0.167\n",
      "epoch 7 | train_loss=0.1683 | train_acc=1.0000\n",
      "             | Val loss=0.3125 AUROC=0.712 AUPRC=0.155\n",
      "epoch 8 | train_loss=0.0747 | train_acc=0.9583\n",
      "             | Val loss=0.3167 AUROC=0.721 AUPRC=0.158\n",
      "epoch 9 | train_loss=0.0677 | train_acc=0.9792\n",
      "             | Val loss=0.3226 AUROC=0.703 AUPRC=0.151\n"
     ]
    }
   ],
   "source": [
    "## Step 6 Putting it all together \n",
    "##File path for training data. \n",
    "\n",
    "csv_path_train = \"~/early-sepsis-prediction/Data/subset_fortesting/subset_test.csv\"\n",
    "csv_path_val_or_test = \"~/early-sepsis-prediction/Data/subset_fortesting/subset_val.csv\"\n",
    "\n",
    "\n",
    "train_df, val_df, means, stds = load_and_normalize(csv_path_train, csv_path_val_or_test)\n",
    "\n",
    "train_patients, in_ch = df_to_patients(train_df)\n",
    "val_patients, _       = df_to_patients(val_df)\n",
    "\n",
    "train_dataset = TCNDataset(train_patients)\n",
    "val_dataset   = TCNDataset(val_patients)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_tcn)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, collate_fn=collate_tcn)\n",
    "\n",
    "## Class imbalance issue: use Weight adjusting to address it\n",
    "num_pos = sum(int(p[\"label\"].item()) for p in train_patients)\n",
    "num_neg = len(train_patients) - num_pos\n",
    "\n",
    "if num_pos == 0:\n",
    "    pos_weight = torch.tensor([1.0])\n",
    "else:\n",
    "    pos_weight = torch.tensor([num_neg / num_pos])\n",
    "\n",
    "#pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32)\n",
    "print(\"pos_weight =\", pos_weight)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TCNModel(in_ch=in_ch, hidden_ch=64, num_levels=4, kernel_size=3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # ----- TRAIN -----\n",
    "    batch_losses = []\n",
    "    batch_accs = []\n",
    "    for batch in train_loader:\n",
    "        loss, acc = train_step(model, batch, optimizer, device, pos_weight)\n",
    "        batch_losses.append(loss)\n",
    "        batch_accs.append(acc)\n",
    "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
    "    avg_acc  = sum(batch_accs) / len(batch_accs)\n",
    "    print(f\"epoch {epoch} | train_loss={avg_loss:.4f} | train_acc={avg_acc:.4f}\")\n",
    "\n",
    "    # ----- Validation/Test -----\n",
    "    val_metrics = eval_epoch(model, val_loader, device)\n",
    "    print(f\"             | Val loss={val_metrics['val_loss']:.4f} \"\n",
    "          f\"AUROC={val_metrics['val_auroc']:.3f} AUPRC={val_metrics['val_auprc']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d1d6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "    all_accs = []\n",
    "\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[\"x\"].to(device)             # [B, C, T]\n",
    "        y = batch[\"y\"].to(device)             # [B]\n",
    "        last_idx = batch[\"last_idx\"].to(device)\n",
    "\n",
    "        logits_time = model(x)                # [B, T]\n",
    "        B = logits_time.shape[0]\n",
    "        logits_last = logits_time[torch.arange(B, device=device), last_idx]  # [B]\n",
    "\n",
    "        loss = loss_fn(logits_last, y)\n",
    "\n",
    "        probs = torch.sigmoid(logits_last)    # [B]\n",
    "        preds = (probs > 0.5).float()\n",
    "        acc = (preds == y).float().mean()\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "        all_accs.append(acc.item())\n",
    "\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_targets.append(y.cpu())\n",
    "\n",
    "    # concat for possible AUROC / etc.\n",
    "    all_probs = torch.cat(all_probs)      # shape [N_val_patients]\n",
    "    all_targets = torch.cat(all_targets)  # shape [N_val_patients]\n",
    "\n",
    "    avg_loss = float(torch.tensor(all_losses).mean())\n",
    "    avg_acc  = float(torch.tensor(all_accs).mean())\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": avg_loss,\n",
    "        \"val_acc\": avg_acc,\n",
    "        \"probs\": all_probs,\n",
    "        \"targets\": all_targets,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a99f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
